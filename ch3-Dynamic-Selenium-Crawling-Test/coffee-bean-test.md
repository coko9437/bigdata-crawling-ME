
---

### 커피빈 매장 정보 크롤링: 코드 분석 및 해설

이 코드는 **커피빈 코리아 웹사이트**에서 전국 매장 정보를 수집하는 자동화 프로그램입니다. Selenium으로 동적인 웹 페이지(팝업창)를 제어하고, BeautifulSoup으로 원하는 정보(매장명, 주소, 전화번호)를 정교하게 추출합니다. 마지막으로, 수집된 데이터를 영구적으로 보관하기 위해 JSON 파일과 MariaDB 데이터베이스에 저장하는 완전한 데이터 파이프라인을 구축합니다.

### ★ 사전 분석: 타겟 웹사이트(커피빈)는 어떻게 동작하는가?

성공적인 크롤링을 위해 코드를 짜기 전, 우리는 먼저 '탐정'이 되어 웹사이트가 어떻게 정보를 보여주는지 파악해야 합니다.

**[탐정의 수사 노트]**

1.  **웹사이트 접속**: `https://www.coffeebeankorea.com/store/store.asp` 에 접속합니다.
2.  **관찰**: 왼쪽에는 매장 목록이, 오른쪽에는 지도가 보입니다. 매장 목록에서 '학동역 DT점'의 '자세히 보기' 버튼에 마우스를 올려보거나, 개발자 도구(F12)로 해당 링크를 살펴보겠습니다.
3.  **결정적 단서 발견**: '자세히 보기' 링크의 HTML을 보면, 일반적인 웹사이트 주소(`href="..."`)가 아니라 `javascript:storePop2('33')` 과 같은 코드가 들어있습니다.

    

4.  **추리**: 이것은 이 링크를 클릭하면 `storePop2`라는 이름의 자바스크립트 함수가 `'33'`이라는 값을 가지고 실행된다는 뜻입니다. 즉, 페이지 이동 없이, **자바스크립트가 화면 위에 팝업창을 띄워** 매장 상세 정보를 보여주는 방식입니다. 이 숫자 '33'은 '학동역 DT점'의 고유 ID로 보입니다.
5.  **팝업창 내부 구조 분석**: `storePop2('33')`을 실행시켜 팝업창을 띄운 뒤, 개발자 도구로 팝업창 내부를 샅샅이 살펴봅니다.
    *   **매장명**: `<div class="store_txt">` 안에 있는 `<h2>` 태그에 들어있습니다.
    *   **주소와 전화번호**: 같은 `<div class="store_txt">` 안에 있는 `<table class="store_table">` 태그 내부에 있습니다. 주소는 세 번째 `<td>`에, 전화번호는 네 번째 `<td>`에 위치하는 것을 확인했습니다.

**결론**: 이 웹사이트를 크롤링하려면, 단순히 링크를 클릭하는 것이 아니라 **Selenium을 이용해 `storePop2()` 자바스크립트 함수를 매장 ID 번호를 바꿔가며 직접 실행**해야 합니다. 그리고 팝업창이 뜨면, 그 내용을 BeautifulSoup으로 분석하여 데이터를 추출하는 전략이 가장 효과적입니다. 이 코드의 작성자는 바로 이 전략을 완벽하게 구사했습니다.

---

### 1. 코드 전체적인 흐름

1.  **크롤링 준비**: Selenium으로 Chrome 브라우저를 실행하고, 나중에 데이터를 담을 빈 리스트(`result`)를 준비합니다.
2.  **반복을 통한 데이터 수집**: `for` 루프를 통해 매장 고유 ID를 1부터 순차적으로 증가시킵니다.
    *   각 ID마다 `storePop2(ID)` 자바스크립트 함수를 실행하여 해당 매장의 상세 정보 팝업창을 띄웁니다.
    *   팝업창이 뜬 상태의 HTML 소스를 가져와 BeautifulSoup으로 분석(파싱)합니다.
    *   미리 분석한 HTML 구조에 따라 매장명, 주소, 전화번호를 정확히 추출합니다.
    *   추출한 데이터를 딕셔너리 형태로 가공하여 `result` 리스트에 차곡차곡 쌓습니다.
    *   혹시 없는 매장 ID거나 오류가 발생하면, 프로그램을 멈추지 않고 다음 ID로 넘어갑니다 (예외 처리).
3.  **크롤링 종료**: 모든 ID를 확인한 후, 브라우저를 닫고 수집된 데이터가 담긴 `result` 리스트를 반환합니다.
4.  **데이터 저장**: `main` 함수에서 크롤링 결과를 받아, `save_to_json` 함수를 호출하여 JSON 파일로 저장하고, `save_to_mariadb` 함수를 호출하여 데이터베이스에 저장합니다.

---

### 2. 코드 상세 설명

#### 2.1. `CoffeeBean_store()`: 데이터 수집의 심장

이 함수는 실제 크롤링 작업을 수행하는 핵심 부분입니다.

*   **분기점**: `[크롤링 시작] -> [크롬 브라우저 실행] -> [매장 번호(1~9) 반복 시작]`

    ```python
    def CoffeeBean_store():
        CoffeeBean_URL = "https://www.coffeebeankorea.com/store/store.asp"
        wd = webdriver.Chrome() # 1. 크롬 브라우저를 연다 (리모컨은 wd)
        result = [] # 2. 데이터를 담을 빈 바구니 준비
    
        for i in range(1, 10):  # 3. 매장 ID를 1부터 9까지 순회 (테스트용)
            wd.get(CoffeeBean_URL) # 4. 매번 루프마다 초기 페이지로 돌아간다
            time.sleep(1)
    ```

*   **분기점**: `[자바스크립트 함수 실행으로 팝업 띄우기] -> [현재 페이지 소스 가져오기] -> [BeautifulSoup으로 분석 준비]`

    ```python
        try:
            # 5. 여기가 핵심! Selenium으로 자바스크립트 함수를 직접 실행
            wd.execute_script(f"storePop2({i})")
            time.sleep(3)  # 팝업창이 뜨고 내용이 채워질 때까지 기다려준다
            
            # 6. 팝업이 뜬 상태의 HTML 전체를 가져온다
            html = wd.page_source
            # 7. 가져온 HTML을 BeautifulSoup에게 넘겨 분석을 맡긴다
            soupCB = BeautifulSoup(html, 'html.parser')
    ```    *   `wd.execute_script(f"storePop2({i})")`: 이 부분이 바로 이 코드의 **가장 똑똑한 부분**입니다. `for` 루프의 `i`값이 1, 2, 3...으로 변할 때마다 `storePop2(1)`, `storePop2(2)`... 를 실행하여 각 매장의 팝업을 순서대로 띄웁니다. 이것은 우리가 사전에 웹사이트를 분석한 결과를 완벽하게 활용한 것입니다.

*   **분기점**: `[매장명 추출] -> [주소/전화번호 추출] -> [결과 리스트에 추가]`

    ```python
            # 8. 매장명 추출 (우리가 분석한 HTML 구조 그대로)
            store_name_h2 = soupCB.select("div.store_txt > h2")
            store_name = store_name_h2[0].string.strip() if store_name_h2 else "정보 없음"
            
            # 9. 유효성 검사: 없는 매장이면 건너뛴다
            if store_name == "정보 없음":
                print(f"[{i}] 매장 정보가 존재하지 않아 건너뜁니다.")
                continue

            # 10. 주소 및 전화번호 추출
            store_info = soupCB.select("div.store_txt > table.store_table > tbody > tr > td")
            store_address = store_info[2].text.strip() # 3번째 td (인덱스 2)
            store_phone = store_info[3].text.strip() # 4번째 td (인덱스 3)

            print(f"📍 매장명: {store_name} | 📍 주소: {store_address} | 📞 전화번호: {store_phone}")
            
            # 11. 추출한 데이터를 딕셔너리로 묶어 바구니(result)에 추가
            result.append({"index": i,"store": store_name, "address": store_address, "phone": store_phone})

        except Exception as e:
            # 12. 어떤 이유로든 오류가 나면 기록하고 다음으로 넘어간다
            print(f"❌ 오류 발생: {e}")
            continue
    ```
    *   `soupCB.select(...)`: CSS 선택자를 이용하여 우리가 원하는 정보의 위치를 정확히 '저격'합니다. 사전 분석 단계에서 파악한 `div.store_txt > h2` 와 같은 경로를 그대로 코드로 옮긴 것입니다.

*   **분기점**: `[반복 종료] -> [브라우저 닫기] -> [최종 결과 반환]`

    ```python
    wd.quit() # 13. 모든 작업이 끝나면 브라우저를 닫는다
    return result # 14. 데이터가 가득 찬 바구니를 반환한다
    ```

#### 2.2. `save_to_json()` & `save_to_mariadb()`: 깔끔한 데이터 저장소

이 함수들은 `CoffeeBean_store`가 수집해온 소중한 데이터를 안전하게 저장하는 역할을 합니다.

*   **`save_to_json`**: 파이썬 리스트/딕셔너리 데이터를 사람이 읽기 좋은 텍스트 파일 형식인 JSON으로 저장합니다. `ensure_ascii=False`는 한글이 깨지지 않게 보존하는 중요한 옵션입니다.
*   **`save_to_mariadb`**:
    *   `pymysql.connect`: 데이터베이스 서버에 접속합니다.
    *   `CREATE TABLE IF NOT EXISTS`: 테이블이 없을 때만 새로 만들어주어, 코드를 여러 번 실행해도 오류가 나지 않도록 하는 실무적인 코드입니다.
    *   `cursor.executemany`: `for` 루프를 돌며 `INSERT`를 여러 번 실행하는 것보다 훨씬 효율적으로 모든 데이터를 한 번에 DB에 삽입합니다.
    *   `try...except...`: 데이터 저장 중 오류가 발생하면 `db.rollback()`을 통해 작업을 모두 취소하여 데이터가 일부만 잘못 들어가는 것을 방지합니다.
    *   `db.close()`: 모든 작업이 끝나면 데이터베이스와의 연결을 반드시 끊어주어 자원을 반납합니다.

#### 2.3. `main()`: 전체 작업의 지휘자

*   `if __name__ == '__main__':` 이 구문은 이 파이썬 파일을 직접 실행했을 때만 `main()` 함수를 호출하라는 의미입니다.
*   `main()` 함수는 크롤링 함수(`CoffeeBean_store`)를 호출하여 데이터를 가져온 뒤, 순서대로 저장 함수들(`save_to_json`, `save_to_mariadb`)을 호출하여 전체 프로세스를 조율하는 지휘자 역할을 합니다.

---

### 3. 주요 학습 포인트

*   **JavaScript 직접 실행이라는 우회로**: 모든 웹사이트가 친절하게 URL 링크를 제공하지 않습니다. 이 코드처럼 자바스크립트 함수를 직접 실행하는 방법을 사용하면, 클릭으로 제어하기 어려운 동적 웹사이트도 효과적으로 크롤링할 수 있습니다. 이는 크롤링의 차원을 한 단계 높여주는 고급 기술입니다.
*   **Selenium과 BeautifulSoup의 환상적인 시너지**: Selenium은 브라우저를 제어하여 동적인 콘텐츠(팝업)를 화면에 '띄우는' 역할을 하고, BeautifulSoup은 그렇게 띄워진 화면의 복잡한 HTML 구조 속에서 원하는 데이터를 '추출'하는 역할을 합니다. 각자의 장점을 극대화한 최고의 조합입니다.
*   **견고한 데이터 처리 파이프라인 구축**: 데이터를 단순히 화면에 출력하고 끝내는 것이 아니라, 표준 파일 형식(JSON)과 관계형 데이터베이스(MariaDB)에 저장하는 전 과정을 다루고 있습니다. 이는 단순한 스크립트를 넘어 하나의 완성된 '데이터 수집 시스템'을 만드는 과정입니다.
*   **반복과 예외 처리의 중요성**: 수백 개의 매장을 크롤링할 때, 중간에 한두 개가 문제가 생겼다고 해서 전체 프로그램이 멈춰서는 안 됩니다. `for` 루프 안에서 `try...except` 구문을 사용하여 오류가 발생한 항목은 건너뛰고 끝까지 작업을 완수하는 능력은 안정적인 크롤러의 필수 조건입니다.