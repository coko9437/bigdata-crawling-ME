

---

### **Python 데이터 수집 및 처리 학습 요약집**

지금까지의 학습은 **API를 이용한 정형 데이터 수집**에서 시작하여, **정적 웹 크롤링을 통한 비정형 데이터 추출**, 그리고 수집한 데이터를 **파일과 데이터베이스에 저장 및 시각화**하는 완전한 데이터 파이프라인을 구축하는 과정이었습니다.

#### **Chapter 1: API를 이용한 데이터 수집 기초**

*   **학습 내용**: Naver 검색 API(블로그, 뉴스)를 활용하여 특정 키워드에 대한 데이터를 수집했습니다.
*   **핵심 기술**:
    *   `urllib.request`: Python 표준 라이브러리를 사용한 기본적인 HTTP 요청 및 응답 처리.
    *   **인증**: API 요청 시 HTTP 헤더에 `X-Naver-Client-Id`, `X-Naver-Client-Secret`를 추가하여 인증.
    *   `json`: API 응답으로 받은 JSON 형식의 데이터를 Python 딕셔너리/리스트로 변환 (파싱).
    *   `csv`: 수집한 데이터를 CSV 파일로 저장하는 기본 방법.
*   **결과물**: 단일 페이지의 API 검색 결과를 가져와 CSV 파일로 저장하는 스크립트.
---
#### **Chapter 2: 정적 웹 크롤링과 BeautifulSoup 기초**

*   **학습 내용**: `BeautifulSoup` 라이브러리를 사용하여 웹 페이지의 HTML 문서를 분석하고 원하는 정보를 추출하는 방법을 배웠습니다.
*   **핵심 기술**:
    *   `BeautifulSoup` 객체 생성: `BeautifulSoup(html, 'html.parser')`.
    *   **태그 찾기**:
        *   `find()`: 조건에 맞는 첫 번째 태그 하나만 찾기.
        *   `find_all()`: 조건에 맞는 모든 태그를 리스트로 찾기. (※`findAll`은 구버전)
    *   **계층적 탐색**: 큰 범위의 태그를 먼저 찾고, 그 안에서 다시 `find()`/`find_all()`을 사용하여 범위를 좁혀나가는 효율적인 탐색 방법.
    *   **데이터 추출**:
        *   `.text`: 태그 안의 텍스트 내용 가져오기.
        *   `['속성명']`: 태그의 `href`, `src` 등 속성 값 가져오기.
*   **결과물**: 웹 페이지(Nate, 로컬 HTML)에서 특정 태그, ID, 클래스를 가진 요소를 찾아 텍스트와 속성을 추출하는 스크립트.
---
#### **Chapter 3: 실전 크롤링과 데이터 처리 파이프라인 구축**

*   **학습 내용**: 공공데이터 API와 알라딘 서점 웹사이트라는 실제 대상을 크롤링하며, 데이터를 수집, 정제, 저장, 시각화하는 종합적인 프로젝트를 수행했습니다.
*   **핵심 기술**:
    *   **문제 해결 능력**: 불규칙한 HTML 구조(`<li>` 태그 개수가 다른 경우)를 `if/elif` 조건문으로 해결.
    *   **페이지네이션**: `for` 루프와 URL 파라미터를 활용해 여러 페이지의 데이터를 자동 수집.
    *   **데이터 저장 다각화**:
        *   `pandas`: DataFrame을 활용한 체계적인 데이터 관리 및 CSV 저장.
        *   `pymysql`: 크롤링한 데이터를 MariaDB/MySQL 데이터베이스에 저장.
        *   `json`: 데이터를 JSON 형식으로 저장.
    *   **데이터 시각화**: `matplotlib`를 사용하여 수집한 통계 데이터를 막대그래프로 시각화.
    *   **코드 안정성**: `try-except`, `commit-rollback`을 통한 데이터베이스 작업의 안정성 확보 및 SSL 보안 설정.
*   **결과물**: 특정 조건의 데이터를 수집하여 CSV/JSON/DB에 저장하고, 그래프로 시각화하는 자동화 프로그램.

---

### **심화 학습 내용 (Advanced Topics)**

지금까지의 학습을 바탕으로 실력을 한 단계 더 끌어올릴 수 있는 중요한 주제들입니다.

#### **1. 동적(Dynamic) 크롤링: Selenium 활용 (중요도: ★★★)**

*   **문제**: 많은 최신 웹사이트들은 페이지에 접속한 후, 자바스크립트(JavaScript)가 실행되어 데이터를 화면에 표시합니다. `urllib`이나 `requests`는 초기 HTML만 가져오므로 이런 데이터를 수집할 수 없습니다. (예: '더보기' 버튼 클릭, 무한 스크롤)
*   **로직/순서도**:
    1.  `Selenium`으로 웹 브라우저(Chrome 등)를 자동으로 실행.
    2.  `driver.get(url)`로 목표 페이지 접속.
    3.  `time.sleep()` 또는 `WebDriverWait`으로 자바스크립트가 로딩될 때까지 **대기**.
    4.  필요 시 `find_element().click()`으로 버튼 클릭, `execute_script()`로 스크롤.
    5.  `driver.page_source`로 최종 렌더링된 HTML 소스를 가져옴.
    6.  가져온 HTML을 `BeautifulSoup`에 전달하여 정적 크롤링과 동일하게 파싱 및 데이터 추출.
*   **핵심**: 사람이 브라우저를 조작하는 행위(클릭, 스크롤, 대기)를 코드로 자동화하는 것입니다.
---
#### **2. HTTP 요청 라이브러리: `requests` 사용 (중요도: ★★★)**

*   **문제**: `urllib`은 기능이 제한적이고 사용법이 직관적이지 않습니다.
*   **로직**: `pip install requests`로 설치 후 `urllib.request`를 `requests`로 대체합니다.
*   **장점**:
    *   **간결함**: `requests.get(url)` 한 줄로 요청 가능.
    *   **파라미터 처리**: `params={'key': 'value'}` 딕셔너리 형태로 전달하면 알아서 URL 인코딩.
    *   **JSON 내장**: `.json()` 메서드로 응답을 바로 Python 딕셔너리로 변환 가능.
    *   **세션 관리**: 로그인 상태를 유지하며 여러 페이지를 크롤링할 때 매우 편리.
*   **정답 (코드 예시)**:
    ```python
    import requests
    response = requests.get('https://api.example.com/data', params={'page': 1, 'size': 10})
    if response.status_code == 200:
        data = response.json()
    ```
---
#### **3. 오류 처리 및 로깅(Logging) 강화 (중요도: ★★☆)**

*   **문제**: 장시간 실행되는 크롤러는 네트워크 오류, 서버 응답 없음, HTML 구조 변경 등 예기치 못한 문제로 중단될 수 있습니다.
*   **로직/순서도**:
    1.  `try-except` 블록을 더 세분화하여 특정 예외(`ConnectionError`, `Timeout`, `JSONDecodeError` 등)를 처리.
    2.  `logging` 라이브러리를 설정하여 `print` 대신 로그를 파일로 기록.
    3.  오류 발생 시, 어떤 URL에서 어떤 오류가 났는지 로그 파일에 기록하고, 다음 작업으로 넘어가도록 처리.
*   **핵심**: 프로그램이 중단되지 않고 안정적으로 계속 실행되게 만들며, 나중에 어떤 문제가 있었는지 추적할 수 있게 합니다.
---
#### **4. 크롤링 예절: `robots.txt`, `time.sleep()`, `User-Agent` (중요도: ★★☆)**

*   **문제**: 너무 잦은 요청은 대상 서버에 부하를 주며, IP가 차단되는 원인이 됩니다.
*   **로직**:
    1.  **`robots.txt` 확인**: 크롤링 전 `http://웹사이트주소/robots.txt`를 방문하여 수집이 허용된 페이지인지 확인.
    2.  **`time.sleep(초)`**: 각 요청 사이에 `time.sleep(1)`처럼 1~3초 정도의 간격을 두어 서버 부하를 줄임.
    3.  **`User-Agent` 변경**: 요청 헤더에 실제 웹 브라우저 정보(`User-Agent`)를 추가하여 크롤러가 아닌 정상적인 사용자로 보이게 설정.
---
#### **5. 데이터베이스 ORM 사용: SQLAlchemy (중요도: ★☆☆)**

*   **문제**: `pymysql`로 직접 SQL 쿼리문을 작성하는 것은 오타가 나기 쉽고, DB 종류가 바뀌면 코드를 수정해야 합니다.
*   **로직**: `SQLAlchemy` 같은 ORM(객체 관계 매핑) 라이브러리를 사용하면, SQL 쿼리 대신 파이썬 클래스와 객체를 통해 DB를 조작할 수 있습니다.
*   **장점**: SQL 문법을 몰라도 DB 작업이 가능하고, 코드의 가독성과 유지보수성이 향상됩니다.
---
#### **6. 페이지네이션(Pagination) 심화 (중요도: ★☆☆)**

*   **문제**: 알라딘처럼 URL 파라미터로 페이징하는 경우 외에, '다음' 버튼을 클릭하거나, 페이지 번호를 직접 클릭해야 하는 경우가 많습니다.
*   **로직**:
    1.  '다음' 버튼에 해당하는 `<a>` 태그를 찾아 `href` 속성을 다음 요청 URL로 사용.
    2.  '다음' 버튼이 비활성화되거나 사라질 때까지 루프를 반복.
    3.  (동적 크롤링) `Selenium`으로 페이지 번호 버튼을 순서대로 클릭.
---
#### **7. 비동기(Asynchronous) 처리: `asyncio` + `aiohttp` (중요도: ★☆☆)**

*   **문제**: 순차적인 크롤링은 하나의 요청이 끝날 때까지 기다려야 하므로 속도가 느립니다.
*   **로직**: `asyncio`와 `aiohttp` 라이브러리를 사용하면, 여러 개의 HTTP 요청을 동시에 보내고 먼저 도착하는 응답부터 처리할 수 있습니다. I/O(네트워크 통신) 대기 시간을 최소화하여 크롤링 속도를 극적으로 향상시킬 수 있습니다. (고급 주제)